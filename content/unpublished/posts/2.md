---
title: "Air Gapped Kubernetes Cluster Provisioning with Cluster API for vSphere"
date: "2020-03-31"
description: ""
categories:
  - "Kubernetes"
tags:
  - "Kubernetes"
  - "Cluster API"
  - "VMware"
menu: side # Optional, add page to a menu. Options: main, side, footer
# Theme-Defined params
#thumbnail: "img/placeholder.jpg" # Thumbnail image
#lead: "Example lead - highlighted near the title" # Lead text
comments: true # Enable Disqus comments for specific page
authorbox: true # Enable authorbox for specific page
toc: true # Enable Table of Contents for specific page
mathjax: true # Enable MathJax for specific page
---

## Introduction

There are many different ways to deploy a Kubernetes cluster.  This post is not to introduce a new methodology 
for deploying new clusters, but to address a common problem that we see in the field.  Whether you are working in 
the defense industry, financial, telecommunications, or some other industry, they all have one thing in common.  That is
they all have a need to protect their data.  One common way to do that is to keep systems isolated from the public internet 
either via a special network that sits behind an enterprise firewall, or by total isolation without the ability to
have any internet connectivity whatsoever.  This makes for challenges in managing those environments as we are in a 
mindset of total connectivity these days.  Specifically, as it relates to Kubernetes, deployments will need the ability 
to pull images and manifests from the internet to accomplish such a feat.  This post shows how to use a common deployment 
tool, [Cluster API](https://github.com/kubernetes-sigs/cluster-api) to deploy the cluster without the ability, or limited 
ability to reach out to the public internet.  I am assuming, since we are completely isolated, that this environment is a 
self-managed vSphere environment.  Although public cloud providers do exist for air-gapped worlds, I feel the most common use 
case for this will involve VMware.  VMware has a provider for Cluster API documented [here](https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/master/docs/getting_started.md).

## Assumptions

This post assumes the following:

- Basic Linux skillset
- Basic VMware skillset
- Basic Kubernetes skillset
- Pre-Existing image registry in the disconnected/air-gapped environment
- Pre-Existing "bootstrap" system with Docker running
- Pre-Existing vSphere environment **(6.7u3 or later)**
- A single internet-connected system to pre-stage artifacts
- A file system with 5G of free space where the pre-stage artifacts will be placed on both connected/disconnected systems
- Approved process to move artifacts from publically connected systems to the target air-gapped environment

## Getting Started

### Connected System Requirements

Before we do anything, we have to assume that there is some system, somewhere, that we can use to procure our artifacts prior to their move into our 
disconnected environment.  Obviously, we can't use osmosis to accomplish this.  :)  This system doesn't have to be able to access our installation environment, 
rather it should simply be thought of as a staging area to download artifacts and collect information about the installation process in the installation 
environment.  The following prerequisite requirements must be met on the system that is internet connected:

1) **Google Storage Utility:** this utility allows us to access the Google Storage API so that we can see and download the 
appropriate Kubernetes OVA templates for VMware.  Please visit [this link](https://cloud.google.com/storage/docs/gsutil_install?hl=th) for
installation instructions.
2) **Docker:** installing Docker onto our connected system allows us an easy way to move images from our connected system into our 
air-gapped environment.  Please visit [this link](https://docs.docker.com/install/) for installation instructions.

### Gather Information

Additionally, we must note several items (e.g. software versions) prior to beginning the download process.  The following prerequisite steps 
must be completed, and noted, following artifact download:

1) **Which Version of Kubernetes:** we will first need to know which version of Kubernetes to deploy.  Organizational constraints may 
restrict us to specific version(s).  The specific versions can be found at <https://github.com/kubernetes/kubernetes/releases>.  

**_NOTE_**
This example moving forward will utilize v1.17.4 as an example.

1) **Physical Location for Artifacts:** the Cluster API installer makes use of Docker images, OVAs, and resource definition files 
in order to install the cluster.  If we are to install a cluster in a disconnected manner, we must manually move these artifacts 
from our connected system into our disconnected environment.  We'll need to note the physical location of where to download these images in our connected environment, 
and where we will upload them to in our disconnected environment.

**_NOTE_**
Please see [Download Artifacts](#download-artifacts) for the expected directory structure/location in this example.

3) **HTTP Location of HAProxy/Node OVAs:** the [image-builder](https://github.com/kubernetes-sigs/image-builder) project produces several OVA images which 
meet all of the necessary requirements for Cluster API to produce a functional working Kubernetes cluster.  It is **highly recommended** to use these 
images.  You will need 2 images:
    1. **HAProxy OVA:** this is the load balancer that is used for kube-apiserver
    2. **Node:** the actual image used to produce Kubernetes nodes

To get a list of images, you can use the `gsutil` command which should have been installed in the [Connected System Requirements](#connected-system-requirements) 
section.

```bash
# set our default kubernetes version
export KUBERNETES_VERSION='v1.17.4'

# list the approved images for kubernetes nodes
gsutil ls gs://capv-images/release/*/*.{ova,sha256} | sed 's~^gs://~http://storage.googleapis.com/~' | grep ${KUBERNETES_VERSION}
# http://storage.googleapis.com/capv-images/release/v1.17.4/centos-7-kube-v1.17.4.ova
# http://storage.googleapis.com/capv-images/release/v1.17.4/photon-3-kube-v1.17.4.ova
# http://storage.googleapis.com/capv-images/release/v1.17.4/ubuntu-1804-kube-v1.17.4.ova
# http://storage.googleapis.com/capv-images/release/v1.17.4/centos-7-kube-v1.17.4.ova.sha256
# http://storage.googleapis.com/capv-images/release/v1.17.4/photon-3-kube-v1.17.4.ova.sha256
# http://storage.googleapis.com/capv-images/release/v1.17.4/ubuntu-1804-kube-v1.17.4.ova.sha256

# list the approved images for haproxy nodes
gsutil ls gs://capv-images/extra/haproxy/release/*/*.{ova,sha256} | sed 's~^gs://~http://storage.googleapis.com/~'
# http://storage.googleapis.com/capv-images/extra/haproxy/release/v0.5.3-77-g224e0ef6/capv-haproxy-v0.5.3-77-g224e0ef6.ova
# http://storage.googleapis.com/capv-images/extra/haproxy/release/v0.6.0-rc.1/capv-haproxy.ova
# http://storage.googleapis.com/capv-images/extra/haproxy/release/v0.6.0-rc.2/capv-haproxy-v0.6.0-rc.2.ova
# http://storage.googleapis.com/capv-images/extra/haproxy/release/v0.6.0/capv-haproxy-v0.6.0.ova
# http://storage.googleapis.com/capv-images/extra/haproxy/release/v0.6.2/capv-haproxy-v0.6.2.ova
# http://storage.googleapis.com/capv-images/extra/haproxy/release/v0.6.3/capv-haproxy-v0.6.3.ova
# http://storage.googleapis.com/capv-images/extra/haproxy/release/v0.5.3-77-g224e0ef6/capv-haproxy-v0.5.3-77-g224e0ef6.ova.sha256
# http://storage.googleapis.com/capv-images/extra/haproxy/release/v0.6.0-rc.1/capv-haproxy.ova.sha256
# http://storage.googleapis.com/capv-images/extra/haproxy/release/v0.6.0/capv-haproxy-v0.6.0.ova.sha256
# http://storage.googleapis.com/capv-images/extra/haproxy/release/v0.6.2/capv-haproxy-v0.6.2.ova.sha256
# http://storage.googleapis.com/capv-images/extra/haproxy/release/v0.6.3/capv-haproxy-v0.6.3.ova.sha256
```

Be aware that the Kubernetes version is a part of the URL and file name.  You should have determined which Kubernetes version to use in the previous 
step and be using it as the `KUBERNETES_VERSION` environment variable when running the `gsutil` command.

### Download Artifacts

Once we can determine the location of the artifacts, both the internet location and the physical location of our connected 
system, we can download them.  But first, we should probably create a structure in which to download our artifacts to that 
it is easily transportable to our air-gapped installation environment.

Each artifact being downloaded below assumes the following directory structure:

```bash
./
├── resources      # folder for storing resource definition files
├── docker-images  # folder for storing docker images
├── binaries       # folder for storing installation binaries
└── ovas           # folder for storing vmware ova templates
```

If you want to follow along, this directory structure can be created easily with the following command (run from the directory 
in which you wish to produce the skeleton structure):

```bash
mkdir ovas docker-images resources binaries
```

#### OVA Template for HAProxy

**Download Directory:** ./ovas

Download the appropriate HA Proxy OVA image as determined from the [previous step](#gather-information):

```bash
# download the ova to a haproxy.ova file to the current working directory
curl -L http://storage.googleapis.com/capv-images/extra/haproxy/release/v0.6.3/capv-haproxy-v0.6.3.ova -o ./ovas/haproxy.ova
```

#### OVA Template for Kubernetes Nodes

**Download Directory:** ./ovas

Download the appropriate Kubernetes node OVA image as determined from the [previous step](#gather-information):

```bash
# set our default kubernetes version
export KUBERNETES_VERSION='v1.17.4'

# download the ova to a node.ova file to the current working directory
curl -L http://storage.googleapis.com/capv-images/release/v1.17.4/ubuntu-1804-kube-${KUBERNETES_VERSION}.ova -o ./ovas/node.ova
```

#### Latest Version of Cluster API

**Download Directory:** ./binaries

Cluster API ships with the `clusterctl` utility.  You will need this in order to interact with your cluster deployment.  Download instructions can be found [here](https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl).  Please also be 
aware of the release version of Cluster API, which are located [here](https://github.com/kubernetes-sigs/cluster-api/releases). For quick reference, to download the utility (for MacOS):

```bash
# set the clusterapi version
CLUSTERAPI_VERSION='v0.3.3'

# download clusterctl
curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/${CLUSTERAPI_VERSION}/clusterctl-darwin-amd64 -o ./binaries/clusterctl
```

#### Cluster API Resource Definitions

**Download Directory:** ./resources/clusterapi

Cluster API makes use of several custom resource definitions to both deploy and manage the lifecycle of Kubernetes clusters.  We will 
need to download these and potentially leverage them during our deployment.  

**NOTE:** currently, the below command points to invalid locations as noted in issue https://github.com/kubernetes-sigs/cluster-api/issues/2830.  You will 
need to manually download the CRD files using the appropriate URLs which can be loosely translated as follows:

```bash
BAD URL: https://github.com/kubernetes-sigs/cluster-api/releases/latest/core-components.yaml
GOOD URL: https://github.com/kubernetes-sigs/cluster-api/releases/download/v0.3.3/core-components.yaml
```

You can download via the following commands:

```bash
# display the custom resource definitions (valid as of clusterapi v0.3.3)
clusterctl config repositories
NAME          TYPE                     URL
cluster-api   CoreProvider             https://github.com/kubernetes-sigs/cluster-api/releases/latest/core-components.yaml
kubeadm       BootstrapProvider        https://github.com/kubernetes-sigs/cluster-api/releases/latest/bootstrap-components.yaml
kubeadm       ControlPlaneProvider     https://github.com/kubernetes-sigs/cluster-api/releases/latest/control-plane-components.yaml
aws           InfrastructureProvider   https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/latest/infrastructure-components.yaml
azure         InfrastructureProvider   https://github.com/kubernetes-sigs/cluster-api-provider-azure/releases/latest/infrastructure-components.yaml
metal3        InfrastructureProvider   https://github.com/metal3-io/cluster-api-provider-metal3/releases/latest/infrastructure-components.yaml
openstack     InfrastructureProvider   https://github.com/kubernetes-sigs/cluster-api-provider-openstack/releases/latest/infrastructure-components.yaml
vsphere       InfrastructureProvider   https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/releases/latest/infrastructure-components.yaml

# download the crds
# NOTE: this must be done manually for now as per the above bolded note.  for reference:
mkdir -p ./resources/clusterapi
cd ./resources/clusterapi
curl -L -O https://github.com/kubernetes-sigs/cluster-api/releases/download/v0.3.3/cluster-api-components.yaml
curl -L -O https://github.com/kubernetes-sigs/cluster-api/releases/download/v0.3.3/core-components.yaml
curl -L -O https://github.com/kubernetes-sigs/cluster-api/releases/download/v0.3.3/bootstrap-components.yaml
curl -L -O https://github.com/kubernetes-sigs/cluster-api/releases/download/v0.3.3/control-plane-components.yaml
curl -L -O https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/releases/download/v0.6.3/infrastructure-components.yaml
```

#### KIND

**Download Directory:** ./binaries (for installation binary)  
**Download Directory:** ./docker-images (for Docker image)

Cluster API makes use of the concept that we will first install a cluster which is used to bootstrap other clusters.  This 
concept is documented in [The Cluster API Book](https://cluster-api.sigs.k8s.io/user/concepts.html).  To do this, 
an often-used tactic is to make use of a local [KIND (Kubernetes in Docker)](https://github.com/kubernetes-sigs/kind) cluster.  This allows a quick spinup of a local management cluster to bootstrap other clusters.

To download the kind binary, run the following commands:

```bash
# download the kind binary
curl -L -o ./binaries/kind "https://github.com/kubernetes-sigs/kind/releases/download/v0.7.0/kind-$(uname)-amd64"
```

To download the kind Docker image, run the following commands:

**_NOTE_** Not all versions of Kubernetes are produced for KIND.  This version can mismatch the Kubernetes version you are wanting to install, as the KIND cluster 
is only used for bootstrapping the actual workload cluster.  Versions of KIND are listed [here](https://hub.docker.com/r/kindest/node/tags).

```bash
# use docker to download the kind image
docker pull kindest/node:v1.17.2

# save the docker kind image as a tar archive
mkdir -p ./docker-images/kind
docker save kindest/node:v1.17.2 > ./docker-images/kind/kind-image.tar
```

#### Networking Plugin Definitions

**Download Directory:** ./resources/cni

For any Kubernetes cluster to be active, it must use an appropriate CNI network plugin.  This is not to debate which is the 
appropriate one for your environment, as there is plenty of opinions already out there.  This is just to note that you will need 
to have the one you decide upon available.

For this walkthrough, we will use Calico:

```bash
mkdir -p ./resources/cni
curl https://docs.projectcalico.org/v3.9/manifests/calico.yaml -L -o ./resources/cni/calico.yaml
```

#### vSphere Storage Resource Definitions

**Download Directory:** ./resources/cpi (for vSphere Cloud Provider Interface components)   
**Download Directory:** ./resources/csi (for vSphere Cloud Storage Interface components)

We will need to ensure that we download the vSphere storage driver if we want to make use of the native storage 
integrations with vSphere.  Although it's not a prerequisite for a functional Kubernetes cluster, it's nice to 
have while we are downloading and moving stuff from connected to disconnected environments, as this is often 
times a slow and painful process.

A full list of instructions can be found at https://cloud-provider-vsphere.sigs.k8s.io/tutorials/kubernetes-on-vsphere-with-kubeadm.html.

To download the appropriate vSphere Cloud Provider definitons:

```bash
mkdir -p ./resources/cpi
cd ./resources/cpi
curl -L -O https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/master/manifests/controller-manager/cloud-controller-manager-roles.yaml
curl -L -O https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/master/manifests/controller-manager/cloud-controller-manager-role-bindings.yaml
curl -L -O https://github.com/kubernetes/cloud-provider-vsphere/raw/master/manifests/controller-manager/vsphere-cloud-controller-manager-ds.yaml
```

To download the appropriate vSphere Storage Provider definitons:

```bash
mkdir -p ./resources/csi
cd ./resources/csi
curl -L -O https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/master/manifests/1.14/rbac/vsphere-csi-controller-rbac.yaml
curl -L -O https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/master/manifests/1.14/deploy/vsphere-csi-controller-ss.yaml
curl -L -O https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/master/manifests/1.14/deploy/vsphere-csi-node-ds.yaml
```

#### Download Cert Manager Resource Definitions

**Download Directory:** ./resources/cert-manager

Cert Manager is an upstream project that allows Kubernetes users the ability to manage certificates within their Kubernetes 
cluster.  It is leveraged by Cluster API.  You can get more information on the project [here](https://cert-manager.io/docs/installation/kubernetes/).

To download the resource definition:

```bash
mkdir -p ./resources/cert-manager
cd ./resources/cert-manager
curl -L -O https://github.com/jetstack/cert-manager/releases/download/v0.14.1/cert-manager.yaml
```

#### Download the Docker Images

**Download Directory:** ./docker-images

This is the part that gets a bit tricky.  We have already downloaded the kind image and we had to do that as a one-off as 
kind is not leveraged by the Cluster API resource definition files, as it expects a pre-existing cluster (which is what kind 
is used for).  However, now we must download all of our images which are referenced in our downloaded definition files.  Cluster 
API will use the `image:` key to define where images are located.  We must find every reference to the `image:` key and download 
the value of that key.

**_DISCLAIMER_** This works in a vacuum, but more testing should be taken when downloading these images in an actual 
production environment.

```bash
for TYPE in $(ls resources/); do
  for IMAGE in $(grep image: resources/${TYPE}/* -r | awk '{print $NF}' | tr -d '"' | sort -u); do 
    # create some useful variables
    COMPONENT=$(echo ${IMAGE} | awk -F'/' '{print $NF}')
    COMPONENT_NAME=$(echo ${COMPONENT} | awk -F':' '{print $1}')
    COMPONENT_VERS=$(echo ${COMPONENT} | awk -F':' '{print $NF}')
    IMAGE_DIRECTORY="./docker-images/${TYPE}"

    # pull the image from the url
    docker pull ${IMAGE}

    # save the image as a tar file
    if [[ ! -d ${IMAGE_DIRECTORY} ]]; then mkdir -p ${IMAGE_DIRECTORY}; fi
    docker save ${IMAGE} > ${IMAGE_DIRECTORY}/${COMPONENT_NAME}.tar
  done
done
```

#### Creating the Tarball

Once everything has been downloaded, you should have a directory structure which looks similar to the following:

```bash
./
├── binaries
│   ├── clusterctl
│   └── kind
├── docker-images
│   ├── cert-manager
│   │   ├── cert-manager-cainjector.tar
│   │   ├── cert-manager-controller.tar
│   │   └── cert-manager-webhook.tar
│   ├── clusterapi
│   │   ├── cluster-api-controller.tar
│   │   ├── kube-rbac-proxy.tar
│   │   ├── kubeadm-bootstrap-controller.tar
│   │   ├── kubeadm-control-plane-controller.tar
│   │   └── manager.tar
│   ├── cni
│   │   ├── cni.tar
│   │   ├── kube-controllers.tar
│   │   ├── node.tar
│   │   └── pod2daemon-flexvol.tar
│   ├── cpi
│   │   └── manager.tar
│   ├── csi
│   │   ├── csi-attacher.tar
│   │   ├── csi-node-driver-registrar.tar
│   │   ├── csi-provisioner.tar
│   │   ├── driver.tar
│   │   ├── livenessprobe.tar
│   │   └── syncer.tar
│   └── kind
│       └── kind.tar
├── kube-cluster-api.tar.gz
├── ovas
│   ├── haproxy.ova
│   └── node.ova
└── resources
    ├── cert-manager
    │   └── cert-manager.yaml
    ├── clusterapi
    │   ├── bootstrap-components.yaml
    │   ├── cluster-api-components.yaml
    │   ├── control-plane-components.yaml
    │   ├── core-components.yaml
    │   └── infrastructure-components.yaml
    ├── cni
    │   └── calico.yaml
    ├── cpi
    │   ├── cloud-controller-manager-role-bindings.yaml
    │   ├── cloud-controller-manager-roles.yaml
    │   └── vsphere-cloud-controller-manager-ds.yaml
    └── csi
        ├── vsphere-csi-controller-rbac.yaml
        ├── vsphere-csi-controller-ss.yaml
        └── vsphere-csi-node-ds.yaml
```

Once you are happy with what you have, you can tar the directory up and prepare it to be moved onto your airgapped network:

```bash
tar czvf kube-cluster-api.tar.gz ./*
```

### Migrate Artifacts to Air Gapped Environment

Most organizations which utilized disconnected environments have processes in place to move artifacts from an environment which is internet 
facing to environments which have little to no internet connectivity.  It will be important to follow those procedures for your organization as
they vary between organizations.

Once the process has been followed and you have the artifact on the air-gapped network we can begin the process of installing the Kubernetes cluster 
using Cluster API.

#### Unpack the Tarball on our Bootstrap System

In a [previous step](#creating-the-tarball) we created a file `kube-cluster-api.tar.gz` which is a self-contained artifact with all of the necessary items 
needed to install a Kubernetes cluster with Cluster API.  We will first need to identify our bootstrap system and move the `kube-cluster-api.tar.gz` file in 
order to prepare installation.  It is assumed that the bootstrap system has access to the image registry, access to the vCenter server and the user 
has credentials to authenticate with both vCenter/Image registry.  Additionally, it is assumed that Docker is running on the bootstrap server.

Once the `kube-cluster-api.tar.gz` file is on the bootstrap system, we can unpack it:

```bash
tar zxvf kube-cluster-api.tar.gz
```

#### Load the Docker Images on our Bootstrap System and Private Registry

First, we will need to import the docker images into our bootstrap system:

**_NOTE_** Prior to this step, we need to make note of our private image registry and we will need to likely login to authenticate 
with it in order to push images to it.

**_DISCLAIMER_** This works in a vacuum, but more testing should be taken when downloading these images in an actual 
production environment.

```bash
REGISTRY='registry.airgap.example.com/airgap'

# login with the disconnected registry
docker login ${REGISTRY}

# upload the images to the registry
for TYPE in $(ls ./docker-images/); do
  for IMAGE in $(find ./docker-images/${TYPE} -type f); do
    # load the image into the docker cache and store the output as a variable
    LOADED_IMAGE=$(docker load < ${IMAGE} | awk '{print $NF}')

    # create some useful variables
    COMPONENT=$(echo ${LOADED_IMAGE} | awk -F'/' '{print $NF}')
    COMPONENT_NAME=$(echo ${COMPONENT} | awk -F':' '{print $1}')
    COMPONENT_VERS=$(echo ${COMPONENT} | awk -F':' '{print $NF}')
    PRIVATE_IMAGE="${REGISTRY}/${TYPE}/${COMPONENT_NAME}:${COMPONENT_VERS}"
    PRIVATE_IMAGE_LATEST="${REGISTRY}/${TYPE}/${COMPONENT_NAME}:latest"

    # tag the image with our private registry information
    docker tag ${LOADED_IMAGE} ${PRIVATE_IMAGE}
    docker tag ${LOADED_IMAGE} ${PRIVATE_IMAGE_LATEST}

    # push the image to the private registry
    docker push ${PRIVATE_IMAGE}
    docker push ${PRIVATE_IMAGE_LATEST}
  done
done
```

#### Start the Bootstrap KIND Cluster

We will next need to start our bootstrap KIND cluster.  Remember, this cluster is simply a local Kubernetes cluster which is responsible for 
bootstrapping our environment in vSphere.

First, we need to copy our `kind` executable from our tarball to our `$PATH`.

```bash
chmod +x ./binaries/kind
cp ./binaries/kind /usr/local/bin
```

Then we can start the kind cluster from our private registry:

```bash
kind create cluster --name vsphere-bootstrap-cluster --image regmliv001.scott.net/airgap/kind/node:latest
```

#### Update Image References to Use Our Private Registry

For each `image:` line occurence in our resources files, we will need to update them to reference the images in our private registry.  This will be heavily 
dependent upon the structure that we've created thus far.  If you've followed along, you should be good, however do note if you've come up with your 
own methodlogy, you should substitute image names/locations as appropriate.

**_NOTE_** this can be accomplished with some `sed` magic

